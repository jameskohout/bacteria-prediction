{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.utils.vis_utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.ensemble import ExtraTreesClassifier\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-28T22:28:27.947778Z","iopub.execute_input":"2022-02-28T22:28:27.948049Z","iopub.status.idle":"2022-02-28T22:28:27.965991Z","shell.execute_reply.started":"2022-02-28T22:28:27.948020Z","shell.execute_reply":"2022-02-28T22:28:27.964996Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\",index_col=\"row_id\")\ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\",index_col=\"row_id\")","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:28:27.968186Z","iopub.execute_input":"2022-02-28T22:28:27.968687Z","iopub.status.idle":"2022-02-28T22:28:56.083538Z","shell.execute_reply.started":"2022-02-28T22:28:27.968641Z","shell.execute_reply":"2022-02-28T22:28:56.082815Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the Data\n\nThe data comes from DNA readings using a spectroscopy technique. A DNA sequence is a finite sequence of letters \"A\",\"T\",\"G\" and \"C\" (a bacteria genome has length around 5 million letters). They take a sequence and sample it in subsequences of length 10 (10-mers) and count how many of each letter appears in that subsequence. \n\nFor example: $ATGCAATGCG \\to A_3 T_2 G_3 C_2$\n\nThis means instead of storing the whole sequence they just store how many of each possible subsequence of length 10.\n\nA few other things also happen in the data:\n\n1.  Random noise is added to these counts which is meant to correspond to mutations, the possibility the same subseuquence is not read completely/read multiple times, etc... Some 10-mers are changed to random values with some error rate.\n2. They subtract a bias term so that the entries in the data correspond to the deviation from a random 10-mer\n\n### First look at the data\nWe need to get an idea of what the data looks like, especially any differences in the training and test sets.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:28:56.084710Z","iopub.execute_input":"2022-02-28T22:28:56.085051Z","iopub.status.idle":"2022-02-28T22:28:56.111099Z","shell.execute_reply.started":"2022-02-28T22:28:56.085021Z","shell.execute_reply":"2022-02-28T22:28:56.110052Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"print(\"Training data: \")\ntrain.describe().transpose()[[\"mean\",\"std\",\"min\",\"max\"]]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:28:56.112246Z","iopub.execute_input":"2022-02-28T22:28:56.112451Z","iopub.status.idle":"2022-02-28T22:28:58.913084Z","shell.execute_reply.started":"2022-02-28T22:28:56.112425Z","shell.execute_reply":"2022-02-28T22:28:58.912170Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"print(\"Testing data: \")\ntrain.describe().transpose()[[\"mean\",\"std\",\"min\",\"max\"]]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:28:58.915186Z","iopub.execute_input":"2022-02-28T22:28:58.915430Z","iopub.status.idle":"2022-02-28T22:29:01.658578Z","shell.execute_reply.started":"2022-02-28T22:28:58.915400Z","shell.execute_reply":"2022-02-28T22:29:01.657841Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"The values don't look continuous, despite the noise/simulated errors. ","metadata":{}},{"cell_type":"code","source":"print(\"The number of unique values in each column is \\n\")\nprint(str(train.nunique()))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-28T22:29:01.659817Z","iopub.execute_input":"2022-02-28T22:29:01.660028Z","iopub.status.idle":"2022-02-28T22:29:02.384970Z","shell.execute_reply.started":"2022-02-28T22:29:01.660003Z","shell.execute_reply":"2022-02-28T22:29:02.384064Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"print(\"The number of duplicated rows in the training data is\", train.duplicated().sum())\nprint(\"The total number of rows is\", train.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:02.386577Z","iopub.execute_input":"2022-02-28T22:29:02.387191Z","iopub.status.idle":"2022-02-28T22:29:03.972580Z","shell.execute_reply.started":"2022-02-28T22:29:02.387146Z","shell.execute_reply":"2022-02-28T22:29:03.971521Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"It isn't really clear why there are duplicates: is it the sampling method (and so useful data) or is it caused by how they simulate error (so shouldn't really be taken into account if the model is to perform well on real data). Removing duplicates could speed up training significantly, but we don't want to lose any info. It's hard to say what the best move here is given we don't know how the distributions of the training data and testing data differ. ","metadata":{}},{"cell_type":"code","source":"print(\"The number of duplicated rows in the testing data is\", test.duplicated().sum())\nprint(\"The total number of rows is\", test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:03.973947Z","iopub.execute_input":"2022-02-28T22:29:03.974261Z","iopub.status.idle":"2022-02-28T22:29:04.811349Z","shell.execute_reply.started":"2022-02-28T22:29:03.974219Z","shell.execute_reply":"2022-02-28T22:29:04.810590Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"### Bacteria in the training set\nLet's look at what bacteria we have in the data set and the proportion of each one.","metadata":{}},{"cell_type":"code","source":"bacteria = sorted(train[\"target\"].unique())\nprint(\"The bacteria in the data set are: \",bacteria)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:04.812424Z","iopub.execute_input":"2022-02-28T22:29:04.812911Z","iopub.status.idle":"2022-02-28T22:29:04.837563Z","shell.execute_reply.started":"2022-02-28T22:29:04.812875Z","shell.execute_reply":"2022-02-28T22:29:04.836724Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"bacteria_counts = train.groupby(\"target\")[\"target\"].count()\nprint(\"Percentage of training set for each bacteria: \")\nprint(100*bacteria_counts/bacteria_counts.sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:04.838697Z","iopub.execute_input":"2022-02-28T22:29:04.838924Z","iopub.status.idle":"2022-02-28T22:29:04.889413Z","shell.execute_reply.started":"2022-02-28T22:29:04.838895Z","shell.execute_reply":"2022-02-28T22:29:04.888606Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"### Displaying some spectra\nIt could be fun to plot some of these spectra, so lets pick some random ones. I wouldn't say this is essential, but its a bit of practice.","metadata":{}},{"cell_type":"code","source":"# Plot some histograms representing some data points\n(n_x,n_y) = (3,3)\n\nnumber_to_plot = n_x*n_y\nsamples = train.sample(number_to_plot)\n\nfig, axs = plt.subplots(n_x,n_y,sharey=True)\nx = 1 + np.arange(286)\n\nfor i in range(n_x):\n    for j in range(n_y):\n        axs[i,j].bar(x,samples.values[i*j][0:286],width=1.5)\n        axs[i,j].title.set_text(\"bacteria: \" + samples.values[i*j][286])\nfig.set_size_inches(3*n_x,3*n_y)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:04.890683Z","iopub.execute_input":"2022-02-28T22:29:04.890926Z","iopub.status.idle":"2022-02-28T22:29:11.800003Z","shell.execute_reply.started":"2022-02-28T22:29:04.890895Z","shell.execute_reply":"2022-02-28T22:29:11.799334Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"It might actually be useful to look at visual features of different rows labelled as the same bacteria.","metadata":{}},{"cell_type":"code","source":"# Now plot some histograms for some points which are in the same class\nplot_bacteria = \"Campylobacter_jejuni\"\n(n_x,n_y) = (3,3)\n\nnumber_to_plot = n_x*n_y\nbacteria_samples = train[train[\"target\"] == plot_bacteria].sample(number_to_plot)\nfig, axs = plt.subplots(n_x,n_y,sharey=True)\nx = 1 + np.arange(286)\n\nfor i in range(n_x):\n    for j in range(n_y):\n        axs[i,j].bar(x,bacteria_samples.values[i*j][0:286],width=1.5)\n        axs[i,j].title.set_text(\"bacteria: \" + bacteria_samples.values[i*j][286])\nfig.set_size_inches(3*n_x,3*n_y)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:11.801388Z","iopub.execute_input":"2022-02-28T22:29:11.801825Z","iopub.status.idle":"2022-02-28T22:29:18.620130Z","shell.execute_reply.started":"2022-02-28T22:29:11.801792Z","shell.execute_reply":"2022-02-28T22:29:18.619577Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"It looks like they have the same shape but some are scaled by some factor. This is probably to do with the way the error is added, but I haven't read that part of the paper in too much detail.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing\nOur preproccessing has a number of steps:\n1. (Optional) Remove duplicate columns \n2. Encode each bacteria as a number 0-9\n3. Split data into training/validation data\n4. Scale the training data so that is has mean 0 and std 1\n5. Scale the validation and testing data by the mean/std computed above\n\nNote on removing duplicates: This may change the distribution of bacteria in the training set (remember they were 10% each). Probably unwise to do this unless an extra weight column is added representing the number of duplicates.","metadata":{}},{"cell_type":"code","source":"def process_data(train,test,drop_duplicates=False):\n    X_train = train.copy()\n    \n    # Drop duplicate columns\n    if drop_duplicates == True:\n        X_train = X_train.drop_duplicates()\n    \n    y_train = X_train.pop(\"target\")\n    \n    # Encode target column with labels 0-9\n    labeler = LabelEncoder()\n    labeler.fit(y_train)\n    y_train_encoded = labeler.transform(y_train)\n    \n    # Split data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train_encoded,test_size = 0.2, train_size = 0.8)\n    \n    # Scale data\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_val = scaler.transform(X_val)\n    \n    # Apply appropriate transformations to testing data so that predictions can be made\n    X_test = scaler.transform(test)\n    \n    return X_train,X_val,X_test,y_train,y_val,labeler\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:18.623235Z","iopub.execute_input":"2022-02-28T22:29:18.623499Z","iopub.status.idle":"2022-02-28T22:29:18.630423Z","shell.execute_reply.started":"2022-02-28T22:29:18.623469Z","shell.execute_reply":"2022-02-28T22:29:18.629820Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"X_train,X_val,X_test,y_train,y_val,labeler = process_data(train,test,drop_duplicates=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:18.631450Z","iopub.execute_input":"2022-02-28T22:29:18.631669Z","iopub.status.idle":"2022-02-28T22:29:20.632501Z","shell.execute_reply.started":"2022-02-28T22:29:18.631643Z","shell.execute_reply":"2022-02-28T22:29:20.631725Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:20.633645Z","iopub.execute_input":"2022-02-28T22:29:20.633870Z","iopub.status.idle":"2022-02-28T22:29:20.640318Z","shell.execute_reply.started":"2022-02-28T22:29:20.633842Z","shell.execute_reply":"2022-02-28T22:29:20.639387Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"# Creating a model\n### Neural Network\nOne of the models they use in the paper is a Neural Network (among others). This probably won't out perform tree based methods for this type of data, but just for fun let's see what kind of performance we get. \n\nThey say they use \"default parameters\" and a 100 layer (!) network. This is definitely a typo, they use the SKLearn defaults which are 2 layers of 100 units each. \n\n","metadata":{}},{"cell_type":"code","source":"def createModelNN(num_layers=2,num_units=256,batch_norm=True,dropout=0.3):\n    model = tf.keras.Sequential()\n    model.add( tf.keras.layers.Input(X_train.shape[1]) )\n    \n    for i in range(num_layers):\n        model.add( tf.keras.layers.Dense(units=num_units) )\n        if batch_norm == True:\n            model.add( tf.keras.layers.BatchNormalization() )\n        if dropout > 0:\n            model.add( tf.keras.layers.Dropout(dropout) )\n        model.add( tf.keras.layers.Activation(\"relu\") )\n        \n    model.add( tf.keras.layers.Dense(units=10,activation=\"softmax\") )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:20.641386Z","iopub.execute_input":"2022-02-28T22:29:20.641598Z","iopub.status.idle":"2022-02-28T22:29:20.652992Z","shell.execute_reply.started":"2022-02-28T22:29:20.641572Z","shell.execute_reply":"2022-02-28T22:29:20.652385Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"We can then build our model so that it will be ready for training. We also need to define the optimizer and loss function for training the model.\n\nWe use the following:\n\n* Number of layers: 2\n* Number of units: 256\n* Dropout on with a dropping probability of 0.3\n* No batch normalization\n* Optimizer: ADAM \n* Loss: \"sparse_categorical_crossentropy\" is the option in Keras which deals with multiclass clasification with our label type","metadata":{}},{"cell_type":"code","source":"model_NN = createModelNN(num_layers=2,num_units=256,batch_norm=False,dropout=0.3)\nmodel_NN.compile(optimizer=\"adam\", loss = \"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel_NN.build()\nprint(model_NN.summary())\n#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:20.654173Z","iopub.execute_input":"2022-02-28T22:29:20.655002Z","iopub.status.idle":"2022-02-28T22:29:20.713587Z","shell.execute_reply.started":"2022-02-28T22:29:20.654925Z","shell.execute_reply":"2022-02-28T22:29:20.712709Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"We will train the model with early stopping:","metadata":{}},{"cell_type":"code","source":"cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.01,patience=10)\n\nhistory = model_NN.fit(X_train,y_train,epochs=100,batch_size=64,validation_data=(X_val,y_val),callbacks=[cb])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:29:20.714926Z","iopub.execute_input":"2022-02-28T22:29:20.715776Z","iopub.status.idle":"2022-02-28T22:32:48.426137Z","shell.execute_reply.started":"2022-02-28T22:29:20.715729Z","shell.execute_reply":"2022-02-28T22:32:48.425250Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"To get an idea of how the training went let's plot its progress through iterations of the training process: ","metadata":{}},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[[\"accuracy\",\"val_accuracy\"]].plot().set(xlabel=\"Epoch\",ylabel=\"Accuracy\")\nhistory_df[[\"loss\",\"val_loss\"]].plot().set(xlabel=\"Epoch\",ylabel=\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:32:48.428123Z","iopub.execute_input":"2022-02-28T22:32:48.428438Z","iopub.status.idle":"2022-02-28T22:32:48.861546Z","shell.execute_reply.started":"2022-02-28T22:32:48.428394Z","shell.execute_reply":"2022-02-28T22:32:48.860934Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"### Extra Trees Classifier\nThe best performing decision tree algorithm they use in their paper is an Extra Trees Classifier. Let's see what performance we can get from this model. We use 500 estimators in the model and default parameters otherwise. ","metadata":{}},{"cell_type":"code","source":"model_ET = ExtraTreesClassifier(n_estimators=500,verbose=1)\nmodel_ET.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:32:48.862642Z","iopub.execute_input":"2022-02-28T22:32:48.862952Z","iopub.status.idle":"2022-02-28T22:38:37.030015Z","shell.execute_reply.started":"2022-02-28T22:32:48.862923Z","shell.execute_reply":"2022-02-28T22:38:37.029127Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"# Model Analysis\nTo analyze each model we will look at the distribution of bacteria predicted as well as which bacteria are mislabelled in the validation set. \n### Neural Network\nLet's first visualize which bacteria are mislabelled by the model in the training and validation sets.","metadata":{}},{"cell_type":"code","source":"val_predictions_NN = model_NN.predict(X_val)\nval_predictions_NN = tf.argmax(val_predictions_NN,axis=1)\ntrain_predictions_NN = model_NN.predict(X_train)\ntrain_predictions_NN = tf.argmax(train_predictions_NN,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:37.031412Z","iopub.execute_input":"2022-02-28T22:38:37.031671Z","iopub.status.idle":"2022-02-28T22:38:44.441350Z","shell.execute_reply.started":"2022-02-28T22:38:37.031641Z","shell.execute_reply":"2022-02-28T22:38:44.440700Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# Get an array of the wrong predictions\nwrong_val_NN = np.where(y_val != val_predictions_NN, 1,0)\nwrong_train_NN = np.where(y_train != train_predictions_NN,1,0)\nprint(\"Number of incorrect predictions in the validation set is \", wrong_val_NN.sum(), \" Fraction incorrect is \", wrong_val_NN.sum()/len(wrong_val_NN))\nprint(\"Number of incorrect predictions in the training set is \", wrong_train_NN.sum(), \" Fraction incorrect is \", wrong_train_NN.sum()/len(wrong_train_NN))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:44.443758Z","iopub.execute_input":"2022-02-28T22:38:44.444386Z","iopub.status.idle":"2022-02-28T22:38:44.454081Z","shell.execute_reply.started":"2022-02-28T22:38:44.444335Z","shell.execute_reply":"2022-02-28T22:38:44.453306Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay(confusion_matrix(y_val,val_predictions_NN)).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:44.455374Z","iopub.execute_input":"2022-02-28T22:38:44.456093Z","iopub.status.idle":"2022-02-28T22:38:45.188165Z","shell.execute_reply.started":"2022-02-28T22:38:44.456046Z","shell.execute_reply":"2022-02-28T22:38:45.187388Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay(confusion_matrix(y_train,train_predictions_NN)).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:45.189471Z","iopub.execute_input":"2022-02-28T22:38:45.190276Z","iopub.status.idle":"2022-02-28T22:38:46.046933Z","shell.execute_reply.started":"2022-02-28T22:38:45.190230Z","shell.execute_reply":"2022-02-28T22:38:46.046029Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"Next we can use the model to predict the labels for the test set.","metadata":{}},{"cell_type":"code","source":"soft_max_predictions_NN = model_NN.predict(X_test)\npredictions_numerical_NN = tf.argmax(input=soft_max_predictions_NN, axis=1)\npredictions_NN = labeler.inverse_transform(predictions_numerical_NN)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:46.048067Z","iopub.execute_input":"2022-02-28T22:38:46.048278Z","iopub.status.idle":"2022-02-28T22:38:49.702798Z","shell.execute_reply.started":"2022-02-28T22:38:46.048252Z","shell.execute_reply":"2022-02-28T22:38:49.702046Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"We don't know the distribution of bacteria in the testing set, but perhaps they are equally distributed as in the training data. We can check what the distribution of predicted labels is at least:","metadata":{}},{"cell_type":"code","source":"test_counts_NN = pd.DataFrame(predictions_NN,columns=[\"target\"]).groupby(\"target\")[\"target\"].count()\nprint(\"Percentage of predictions in the test set for each bacteria: \")\nprint(100*test_counts_NN/test_counts_NN.sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:49.704127Z","iopub.execute_input":"2022-02-28T22:38:49.705171Z","iopub.status.idle":"2022-02-28T22:38:49.736361Z","shell.execute_reply.started":"2022-02-28T22:38:49.705133Z","shell.execute_reply":"2022-02-28T22:38:49.735474Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"Predictions don't look too far off, although we might be under/overpredicting some classes. ","metadata":{}},{"cell_type":"markdown","source":"### Extra Trees","metadata":{}},{"cell_type":"markdown","source":"We'll repeat the same proccess here to get an idea of the differences of each model. First we look at the predictions on the validation/training set:","metadata":{}},{"cell_type":"code","source":"val_predictions_ET = model_ET.predict(X_val)\ntrain_predictions_ET = model_ET.predict(X_train)\nval_accuracy_ET = model_ET.score(X_val,y_val)\ntrain_accuracy_ET = model_ET.score(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:38:49.737527Z","iopub.execute_input":"2022-02-28T22:38:49.738065Z","iopub.status.idle":"2022-02-28T22:40:30.196766Z","shell.execute_reply.started":"2022-02-28T22:38:49.738019Z","shell.execute_reply":"2022-02-28T22:40:30.195692Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"wrong_val = np.where(y_val != val_predictions_ET, 1,0)\nprint(\"Number of incorrect predictions in the validation set is \", wrong_val.sum(), \" Accuracy is \", val_accuracy_ET)\nwrong_train = np.where(y_train != train_predictions_ET,1,0)\nprint(\"Number of incorrect predictions in the training set is \", wrong_train.sum(), \" Accuracy is \", train_accuracy_ET)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:30.198257Z","iopub.execute_input":"2022-02-28T22:40:30.198594Z","iopub.status.idle":"2022-02-28T22:40:30.207423Z","shell.execute_reply.started":"2022-02-28T22:40:30.198535Z","shell.execute_reply":"2022-02-28T22:40:30.206484Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay(confusion_matrix(y_val,val_predictions_ET)).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:30.209034Z","iopub.execute_input":"2022-02-28T22:40:30.209338Z","iopub.status.idle":"2022-02-28T22:40:30.986884Z","shell.execute_reply.started":"2022-02-28T22:40:30.209295Z","shell.execute_reply":"2022-02-28T22:40:30.986011Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay(confusion_matrix(y_train,train_predictions_ET)).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:30.988350Z","iopub.execute_input":"2022-02-28T22:40:30.989199Z","iopub.status.idle":"2022-02-28T22:40:31.900172Z","shell.execute_reply.started":"2022-02-28T22:40:30.989146Z","shell.execute_reply":"2022-02-28T22:40:31.899508Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"Now we look at the distribution of predictions of the testing data:","metadata":{}},{"cell_type":"code","source":"predictions_numerical_ET = model_ET.predict(X_test)\npredictions_ET = labeler.inverse_transform(predictions_numerical_ET)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:31.901452Z","iopub.execute_input":"2022-02-28T22:40:31.902356Z","iopub.status.idle":"2022-02-28T22:40:57.032908Z","shell.execute_reply.started":"2022-02-28T22:40:31.902314Z","shell.execute_reply":"2022-02-28T22:40:57.031811Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"test_counts_ET = pd.DataFrame(predictions_ET,columns=[\"target\"]).groupby(\"target\")[\"target\"].count()\nprint(\"Percentage of predictions in the test set for each bacteria: \")\nprint(100*test_counts_ET/test_counts_ET.sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:57.034243Z","iopub.execute_input":"2022-02-28T22:40:57.034936Z","iopub.status.idle":"2022-02-28T22:40:57.065841Z","shell.execute_reply.started":"2022-02-28T22:40:57.034881Z","shell.execute_reply":"2022-02-28T22:40:57.064868Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"Assuming the test data are distributed equally (which seems to be the case) we are probably under predicting E. coli.","metadata":{}},{"cell_type":"markdown","source":"# Submission\nTo finish off we will export our predictions so that they can be submitted to the competition for ranking. ","metadata":{}},{"cell_type":"code","source":"submission_NN = pd.DataFrame(predictions_NN, columns=[\"target\"],index=test.index)\nsubmission_NN.to_csv(\"submissionNN.csv\",index=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:57.067285Z","iopub.execute_input":"2022-02-28T22:40:57.067615Z","iopub.status.idle":"2022-02-28T22:40:57.366285Z","shell.execute_reply.started":"2022-02-28T22:40:57.067570Z","shell.execute_reply":"2022-02-28T22:40:57.365175Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"submission_ET = pd.DataFrame(predictions_ET, columns=[\"target\"],index=test.index)\nsubmission_ET.to_csv(\"submissionET.csv\",index=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T22:40:57.367424Z","iopub.execute_input":"2022-02-28T22:40:57.367666Z","iopub.status.idle":"2022-02-28T22:40:57.658435Z","shell.execute_reply.started":"2022-02-28T22:40:57.367637Z","shell.execute_reply":"2022-02-28T22:40:57.657541Z"},"trusted":true},"execution_count":107,"outputs":[]}]}